# Entropy-RAG
Entropy-RAG: Rethinking Retrieval-Augmented Generation from the Perspective of Entropy

## Abstract
Retrieval-Augmented Generation (RAG) has proven effective for augmenting large language models (LLMs) by leveraging external information. However, existing approaches often lack a principled mechanism to dynamically assess when and how to retrieve useful content. In this work, we introduce **Entropy-RAG**, a framework that integrates entropy across the entire retrieval and generation pipeline to optimize both the necessity and quality of retrieval. Our approach introduces three key components: (1) **Entropy-Awareness**, which leverages the entropy of the generated response to determine whether retrieval is required; (2) **Entropy-Retrieval**, which identifies the most uncertain tokens in the response to guide the generation of targeted retrieval queries; and (3) **Entropy-Reranking**, where retrieved contexts are re-ranked based on their entropy reduction, ensuring that only the most informative contexts contribute to the final response. Through comprehensive evaluation on multiple standard benchmarks, we demonstrate that Entropy-RAG significantly improves retrieval relevance and the overall quality of generated responses. Our results show a substantial improvement in both the stability and accuracy of generative outputs, positioning Entropy-RAG as a powerful framework for optimizing retrieval in generative models. Our code is publicly available at [https://github.com/Nomothings/Entropy-RAG](https://github.com/Nomothings/Entropy-RAG).
